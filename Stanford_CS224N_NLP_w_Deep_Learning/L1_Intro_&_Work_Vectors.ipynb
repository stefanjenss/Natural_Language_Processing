{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS224N - Intro & Word Vectors (Lecture 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common NLP solution:** Use, e.g., WordNet, a thesaurus containing lists of **synonym sets** and **hypernyms** (\"is a\" relationships):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/stefanjenss/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun: good\n",
      "noun: good, goodness\n",
      "noun: good, goodness\n",
      "noun: commodity, trade_good, good\n",
      "adj: good\n",
      "adj (s): full, good\n",
      "adj: good\n",
      "adj (s): estimable, good, honorable, respectable\n",
      "adj (s): beneficial, good\n",
      "adj (s): good\n",
      "adj (s): good, just, upright\n",
      "adj (s): adept, expert, good, practiced, proficient, skillful, skilful\n",
      "adj (s): good\n",
      "adj (s): dear, good, near\n",
      "adj (s): dependable, good, safe, secure\n",
      "adj (s): good, right, ripe\n",
      "adj (s): good, well\n",
      "adj (s): effective, good, in_effect, in_force\n",
      "adj (s): good\n",
      "adj (s): good, serious\n",
      "adj (s): good, sound\n",
      "adj (s): good, salutary\n",
      "adj (s): good, honest\n",
      "adj (s): good, undecomposed, unspoiled, unspoilt\n",
      "adj (s): good\n",
      "adv: well, good\n",
      "adv: thoroughly, soundly, good\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# e.g., synonym sets containing \"good\":\n",
    "from nltk.corpus import wordnet as wn\n",
    "poses = { 'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv' }\n",
    "for synset in wn.synsets('good'):\n",
    "    print('{}: {}'.format(poses[synset.pos()],\n",
    "            \", \".join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g., synonyms of \"panda\":\n",
    "from nltk.corpus import wordnet as wn\n",
    "panda = wn.synset('panda.n.01')\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with resources like WordNet\n",
    "- Great as a resource but missing nuance\n",
    "    - e.g., \"proficient\" is listed as a synonym for \"good\"\n",
    "        - This is only correct in some contexts\n",
    "- Missing new meanings of words\n",
    "    - e.g., wicked, badass, nifty, wizard, genius, ninja, bombest\n",
    "    - Impossible to keep up-to-date\n",
    "_ Subjective\n",
    "- Requires human labor to create and adapt\n",
    "- Can't compute acccurate word similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing words as discrete symbols\n",
    "- In traditional NLP, we regard words as discrete symbols: hotel, conference, motel - a localist represetnations\n",
    "- Such symbols for wrods can be represented by one-hot vectors\n",
    "\n",
    "### Representing words by their context\n",
    "- ***Distributional semantics:*** **A word's meaning is given by the words that frequently appear close-by**\n",
    "    - When a word *w* appears in a text, its **context** is the set of words that appear nearby (within a fixed-size window).\n",
    "    - These ***context words*** will represent **banking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors\n",
    "We will build a dense vector for each word, choen so that it is similar to vectors of words that appear in similar contexts\n",
    "\n",
    "- Note: ***word vectors*** are called ***word embeddings*** or ***(neural) word represetnations***\n",
    "    - They are a ***distributed*** representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec: Overview\n",
    "***Word2Vec*** (Mikolov et al. 2013) is a grameword for learning word vectors\n",
    "Idea:\n",
    "- We have a large corpus (\"body\") of text\n",
    "- Every word in a fixed vocabulary is represented by a ***vector***\n",
    "- Go through each position *t* in the text, which has a center word *c* and context (\"outside\") word *o*\n",
    "- Use the ***similarity of the word vectors*** for *c* and *o* to ***calculate the probability*** of *o* given *c* (or visa versa)\n",
    "- ***Keep adjusting the word vectors*** to maximiize this probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim word vector vissualization of various word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/stefanjenss/anaconda3/envs/portfolio/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/stefanjenss/anaconda3/envs/portfolio/lib/python3.10/site-packages (from gensim) (1.11.4)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Using cached smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Downloading gensim-4.3.2-cp310-cp310-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.16.0-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: wrapt, smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-7.0.4 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Instal the gensim package into the current Jupyter kernel\n",
    "!pip install gensim\n",
    "\n",
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For looking at word vecgtors, use Gensim We also use it for word vectors. Gensim isn't really a deep learning package. It's a package for word and text similarity modeling, which started with (LDA-style) topic models and grew into SVD and neural word representations. But its efficient and scalable, and quite widely used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford has a homegrown offering of GloVe word vectors. Gensim provides a library of several sets of word vectors that you can easily load. You can find out more about GloVe on the Glove page. The professor uses the 100d vectors below as a balance between speed and smallness vs. quality. If you try out the 50d vectors, they basically word for similarity but clearly aren't as good for analogy problems. If you load the 300d vectors, you'll wait longer, but they're even better than the 100d vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n"
     ]
    }
   ],
   "source": [
    "model = api.load(\"glove-wiki-gigaword-100\")\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.66146  ,  0.94335  , -0.72214  ,  0.17403  , -0.42524  ,\n",
       "        0.36303  ,  1.0135   , -0.14802  ,  0.25817  , -0.20326  ,\n",
       "       -0.64338  ,  0.16632  ,  0.61518  ,  1.397    , -0.094506 ,\n",
       "        0.0041843, -0.18976  , -0.55421  , -0.39371  , -0.22501  ,\n",
       "       -0.34643  ,  0.32076  ,  0.34395  , -0.7034   ,  0.23932  ,\n",
       "        0.69951  , -0.16461  , -0.31819  , -0.34034  , -0.44906  ,\n",
       "       -0.069667 ,  0.35348  ,  0.17498  , -0.95057  , -0.2209   ,\n",
       "        1.0647   ,  0.23231  ,  0.32569  ,  0.47662  , -1.1206   ,\n",
       "        0.28168  , -0.75172  , -0.54654  , -0.66337  ,  0.34804  ,\n",
       "       -0.69058  , -0.77092  , -0.40167  , -0.069351 , -0.049238 ,\n",
       "       -0.39351  ,  0.16735  , -0.14512  ,  1.0083   , -1.0608   ,\n",
       "       -0.87314  , -0.29339  ,  0.68278  ,  0.61634  , -0.088844 ,\n",
       "        0.88094  ,  0.099809 , -0.27161  , -0.58026  ,  0.50364  ,\n",
       "       -0.93814  ,  0.67576  , -0.43124  , -0.10517  , -1.2404   ,\n",
       "       -0.74353  ,  0.28637  ,  0.29012  ,  0.89377  ,  0.67406  ,\n",
       "        0.86422  , -0.30693  , -0.14718  ,  0.078353 ,  0.74013  ,\n",
       "        0.32658  , -0.052579 , -1.1665   ,  0.87079  , -0.69402  ,\n",
       "       -0.75977  , -0.37164  , -0.11887  ,  0.18551  ,  0.041883 ,\n",
       "        0.59352  ,  0.30519  , -0.54819  , -0.29424  , -1.4912   ,\n",
       "       -1.6548   ,  0.98982  ,  0.27325  ,  1.009    ,  0.94544  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['bread']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25144  ,  0.52157  , -0.75452  ,  0.28039  , -0.31388  ,\n",
       "        0.274    ,  1.1971   , -0.10519  ,  0.82544  , -0.33398  ,\n",
       "       -0.21417  ,  0.22216  ,  0.14982  ,  0.47384  ,  0.41984  ,\n",
       "        0.69397  , -0.25999  , -0.44414  ,  0.58296  , -0.30851  ,\n",
       "       -0.076455 ,  0.33468  ,  0.28055  , -0.99012  ,  0.30349  ,\n",
       "        0.39128  ,  0.031526 , -0.095395 , -0.004745 , -0.81347  ,\n",
       "        0.27869  , -0.1812   ,  0.14632  , -0.42186  ,  0.13857  ,\n",
       "        1.139    ,  0.14925  , -0.051459 ,  0.37875  , -0.2613   ,\n",
       "        0.011081 , -0.28881  , -0.38662  , -0.3135   , -0.1954   ,\n",
       "        0.19248  , -0.52995  , -0.40674  , -0.25159  ,  0.06272  ,\n",
       "       -0.32724  ,  0.28374  , -0.2155   , -0.061832 , -0.50134  ,\n",
       "        0.0093959,  0.30715  ,  0.3873   , -0.74554  , -0.45947  ,\n",
       "        0.40032  , -0.1378   , -0.26968  , -0.3946   , -0.64876  ,\n",
       "       -0.47149  , -0.085536 ,  0.092795 , -0.034018 , -0.61906  ,\n",
       "        0.19123  ,  0.20563  ,  0.29056  , -0.010908 ,  0.15313  ,\n",
       "        0.33144  ,  0.33806  ,  0.061708 ,  0.20785  ,  0.65348  ,\n",
       "       -0.053222 ,  0.18589  ,  0.32647  , -0.11923  ,  0.42008  ,\n",
       "       -0.26931  ,  0.025489 ,  0.0036535,  0.1327   , -0.22763  ,\n",
       "        0.07564  ,  0.55773  ,  0.2978   ,  0.28144  ,  0.19775  ,\n",
       "       -0.23582  ,  0.65303  ,  0.089897 ,  0.35844  ,  0.14304  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['croissant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('canada', 0.6544383764266968),\n",
       " ('america', 0.645224392414093),\n",
       " ('u.s.a.', 0.6184033751487732),\n",
       " ('united', 0.6017189621925354),\n",
       " ('states', 0.5970699191093445),\n",
       " ('australia', 0.5838716626167297),\n",
       " ('world', 0.5590084195137024),\n",
       " ('2010', 0.558070182800293),\n",
       " ('2012', 0.5504006743431091),\n",
       " ('davis', 0.5464468002319336)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('usa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coconut', 0.7097253799438477),\n",
       " ('mango', 0.705482542514801),\n",
       " ('bananas', 0.6887733936309814),\n",
       " ('potato', 0.6629635691642761),\n",
       " ('pineapple', 0.6534532308578491),\n",
       " ('fruit', 0.6519854664802551),\n",
       " ('peanut', 0.6420575976371765),\n",
       " ('pecan', 0.6349172592163086),\n",
       " ('cashew', 0.6294420957565308),\n",
       " ('papaya', 0.6246591210365295)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('banana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('croissants', 0.682984471321106),\n",
       " ('brioche', 0.6283302903175354),\n",
       " ('baguette', 0.5968102812767029),\n",
       " ('focaccia', 0.5876684188842773),\n",
       " ('pudding', 0.5803956389427185),\n",
       " ('souffle', 0.5614769458770752),\n",
       " ('baguettes', 0.5558240413665771),\n",
       " ('tortilla', 0.5449503064155579),\n",
       " ('pastries', 0.5427730679512024),\n",
       " ('calzone', 0.5374531745910645)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('croissant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shunichi', 0.49618101119995117),\n",
       " ('ieronymos', 0.4736502170562744),\n",
       " ('pengrowth', 0.4668096601963043),\n",
       " ('höss', 0.4636845886707306),\n",
       " ('damaskinos', 0.4617849290370941),\n",
       " ('yadin', 0.4617374837398529),\n",
       " ('hundertwasser', 0.4588957726955414),\n",
       " ('ncpa', 0.4577339291572571),\n",
       " ('maccormac', 0.4566109776496887),\n",
       " ('rothfeld', 0.4523947536945343)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(negative='banana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.7699\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1: x2 :: y1 :: returned\n",
    "def analogy(x1, x2, y1):\n",
    "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'monarch'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('man', 'king', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'woman'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('king', 'man', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
