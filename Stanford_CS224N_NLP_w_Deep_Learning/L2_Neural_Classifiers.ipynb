{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS224N - Neural Classifiers (Lecture 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Review: Main idea of word2vec\n",
    "- Start with random word vectors\n",
    "- Iterate through each word in the whoel corpus\n",
    "- Try to predict surrounding words using word vectors\n",
    "- **Learning**: Update vectors so they can predict actual surrounding words better\n",
    "- Doing no more than this, this algorithm learns word vectors that capture well word similarity and meaningful directios in a wordspace!\n",
    "\n",
    "</br>\n",
    "\n",
    "- Word2Vec maximizes objective function by putting similar words nearby in space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimizaiton: Gradient Descent\n",
    "- To learn good word vectors: We have a cost function j(0) we want to minimize.\n",
    "- **Gradient Descent** is an algorithm to minimize j(0) by chainging 0\n",
    "- **Idea**: from current value of 0, calculate gradient of j(0), then take ***small** step in the direction of negative gradient*. Repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
